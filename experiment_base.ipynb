{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m image_extensions:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# Check if the file exists before opening it\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m         img_type \u001b[38;5;241m=\u001b[39m \u001b[43mimghdr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m img_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not an image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Andrea Dayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\imghdr.py:16\u001b[0m, in \u001b[0;36mwhat\u001b[1;34m(file, h)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, (\u001b[38;5;28mstr\u001b[39m, PathLike)):\n\u001b[1;32m---> 16\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         h \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import imghdr\n",
    "import os\n",
    "\n",
    "data_dir = \"Plant Disease Recognition Dataset\"\n",
    "image_extensions = [\".png\", \".jpg\"]  # add all your image file extensions here\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "\n",
    "for filepath in Path(data_dir).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in image_extensions:\n",
    "        if filepath.exists():  # Check if the file exists before opening it\n",
    "            img_type = imghdr.what(filepath)\n",
    "            if img_type is None:\n",
    "                print(f\"{filepath} is not an image\")\n",
    "                os.remove(filepath)\n",
    "            elif img_type not in img_type_accepted_by_tf:\n",
    "                print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")\n",
    "                os.remove(filepath)\n",
    "        else:\n",
    "            print(f\"{filepath} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Load your dataset using ImageDataGenerator\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Plant Disease Recognition Dataset/Train\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Plant Disease Recognition Dataset/Validation\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    seed=1337,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "\n",
    "# Count the number of samples per class in the training set\n",
    "for class_name in class_names:\n",
    "    num_samples = len([filename for filename in train_ds.file_paths if class_name in filename])\n",
    "    print(f\"Number of {class_name} samples in the training set: {num_samples}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Count the number of samples per class in the validation set\n",
    "for class_name in class_names:\n",
    "    num_samples = len([filename for filename in val_ds.file_paths if class_name in filename])\n",
    "    print(f\"Number of {class_name} samples in the validation set: {num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(20):\n",
    "        ax = plt.subplot(5,4,i+1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[np.argmax(labels[i].numpy())])\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding max pooling layers helps reduce the spatial dimensions and control overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load ResNet50 as a base model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create your model\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "resnet_input = preprocess_input(inputs)\n",
    "\n",
    "# Include the output of the ResNet50 base model\n",
    "outputs = base_model(resnet_input)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "\n",
    "visualkeras.layered_view(model, legend=True) # without custom font\n",
    "from PIL import ImageFont\n",
    "visualkeras.layered_view(model, legend=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='d_trial5_rerun.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: pip install tensorflow-addons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "num_classes = len(train_ds.class_names)\n",
    "kappa_metric = tfa.metrics.CohenKappa(num_classes=num_classes, weightage='quadratic')\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Stop training when validation loss does not improve\n",
    "    patience=5,           # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "custom_optimizer = Adam(learning_rate=0.005)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=custom_optimizer,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), kappa_metric],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[early_stopping, LearningRateScheduler(lr_scheduler)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Training history\n",
    "train_loss = history.history['loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "train_precision = history.history['precision']\n",
    "train_recall = history.history['recall']\n",
    "train_auc = history.history['auc']\n",
    "train_kappa = history.history['cohen_kappa']\n",
    "\n",
    "# Validation history\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "val_precision = history.history['val_precision']\n",
    "val_recall = history.history['val_recall']\n",
    "val_auc = history.history['val_auc']\n",
    "val_kappa = history.history['val_cohen_kappa']\n",
    "\n",
    "# Get the best epoch for each metric\n",
    "best_epoch_loss = np.argmin(val_loss)\n",
    "best_epoch_accuracy = np.argmax(val_accuracy)\n",
    "best_epoch_precision = np.argmax(val_precision)\n",
    "best_epoch_recall = np.argmax(val_recall)\n",
    "best_epoch_auc = np.argmax(val_auc)\n",
    "best_epoch_kappa = np.argmax(val_kappa)\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(train_loss, label='Training Loss')\n",
    "axes[0, 0].plot(val_loss, label='Validation Loss')\n",
    "axes[0, 0].set_title('Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].axvline(x=best_epoch_loss, color='r', linestyle='--', label=f'Best Epoch Loss: {best_epoch_loss}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(train_accuracy, label='Training Accuracy')\n",
    "axes[0, 1].plot(val_accuracy, label='Validation Accuracy')\n",
    "axes[0, 1].set_title('Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].axvline(x=best_epoch_accuracy, color='r', linestyle='--', label=f'Best Epoch Accuracy: {best_epoch_accuracy}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(train_precision, label='Training Precision')\n",
    "axes[1, 0].plot(val_precision, label='Validation Precision')\n",
    "axes[1, 0].set_title('Precision')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].axvline(x=best_epoch_precision, color='r', linestyle='--', label=f'Best Epoch Precision: {best_epoch_precision}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(train_recall, label='Training Recall')\n",
    "axes[1, 1].plot(val_recall, label='Validation Recall')\n",
    "axes[1, 1].set_title('Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].axvline(x=best_epoch_recall, color='r', linestyle='--', label=f'Best Epoch Recall: {best_epoch_recall}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# AUC\n",
    "axes[2, 0].plot(train_auc, label='Training AUC')\n",
    "axes[2, 0].plot(val_auc, label='Validation AUC')\n",
    "axes[2, 0].set_title('AUC')\n",
    "axes[2, 0].set_xlabel('Epoch')\n",
    "axes[2, 0].set_ylabel('AUC')\n",
    "axes[2, 0].axvline(x=best_epoch_auc, color='r', linestyle='--', label=f'Best Epoch AUC: {best_epoch_auc}')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# Cohen's Kappa\n",
    "axes[2, 1].plot(train_kappa, label='Training Cohen\\'s Kappa')\n",
    "axes[2, 1].plot(val_kappa, label='Validation Cohen\\'s Kappa')\n",
    "axes[2, 1].set_title('Cohen\\'s Kappa')\n",
    "axes[2, 1].set_xlabel('Epoch')\n",
    "axes[2, 1].set_ylabel('Cohen\\'s Kappa')\n",
    "axes[2, 1].axvline(x=best_epoch_kappa, color='r', linestyle='--', label=f'Best Epoch Cohen\\'s Kappa: {best_epoch_kappa}')\n",
    "axes[2, 1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best epochs\n",
    "print(f'Best Epoch for Validation Loss: {best_epoch_loss}')\n",
    "print(f'Best Epoch for Validation Accuracy: {best_epoch_accuracy}')\n",
    "print(f'Best Epoch for Validation Precision: {best_epoch_precision}')\n",
    "print(f'Best Epoch for Validation Recall: {best_epoch_recall}')\n",
    "print(f'Best Epoch for Validation AUC: {best_epoch_auc}')\n",
    "print(f'Best Epoch for Validation Kappa: {best_epoch_kappa}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy, val_precision, val_recall, val_auc, val_kappa = model.evaluate(val_ds)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation AUC: {val_auc}\")\n",
    "print(f\"Validation Cohen's Kappa: {val_kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Plant Disease Recognition Dataset/Test\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_auc, test_kappa = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test AUC: {test_auc}\")\n",
    "print(f\"Test Cohen's Kappa: {test_kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(model,img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array((images[i].numpy()))\n",
    "    img_array = tf.expand_dims(img_array,0)     # create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100*(np.max(predictions[0])),2)\n",
    "    \n",
    "    return predicted_class , confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Plant Disease Recognition Dataset/Test\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 25))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(20):\n",
    "        ax = plt.subplot(5, 4, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "\n",
    "        # Convert one-hot encoded label to an integer\n",
    "        actual_class_index = np.argmax(labels[i])\n",
    "\n",
    "        # Use the integer label to get the class name\n",
    "        actual_class = class_names[actual_class_index]\n",
    "\n",
    "        # Make predictions\n",
    "        predicted_class, confidence = Prediction(model, images[i].numpy())\n",
    "\n",
    "        plt.title(f\"Actual: {actual_class}\\n Predicted: {predicted_class}\\n Confidence: {confidence}%\")\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "\n",
    "# Get true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    true_labels.extend(np.argmax(labels.numpy(), axis=1))  # True labels\n",
    "    predicted_labels.extend(\n",
    "        np.argmax(model.predict(images), axis=1)\n",
    "    )  # Predicted labels\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Adding labels to the confusion matrix\n",
    "classes = test_ds.class_names\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Display the values in the confusion matrix\n",
    "thresh = cm.max() / 2.0\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(\n",
    "        j,\n",
    "        i,\n",
    "        format(cm[i, j], \"d\"),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
